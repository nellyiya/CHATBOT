{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAiH1SvkaJbU"
   },
   "source": [
    "This chatbot is designed specifically for the agriculture domain. It serves as a virtual assistant for farmers and agricultural workers, providing accurate and timely information about crops, soil health, pest management, and sustainable farming practices. The chatbot helps address the knowledge gap many farmers face due to limited access to experts and resources.\n",
    "\n",
    "By offering real-time advice, weather updates, and actionable tips, the chatbot empowers users to make informed decisions that improve crop yields and reduce losses. It simplifies complex agricultural knowledge into easy-to-understand guidance accessible anytime via chat.\n",
    "\n",
    "The relevance of this chatbot lies in its ability to support farmers in remote or underserved areas, where traditional agricultural extension services are scarce or inefficient. Ultimately, this chatbot enhances agricultural productivity, promotes sustainable practices, and contributes positively to food security and rural livelihoods.\n",
    "\n",
    "Additionally, this chatbot consistently uses a short Q&A style for communication. This means it provides concise, direct answers rather than lengthy explanations, making information quick and easy to grasp. This style ensures that farmers can quickly get the help they need without confusion or delay.\n",
    "\n",
    "I developed this chatbot because farmers need reliable, round-the-clock assistance, and this tool helps fill that crucial gap with scalable, efficient support.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdKgD_9_aTZe"
   },
   "source": [
    "The KisanVaani agriculture QA dataset contains question-answer pairs focused on practical farming topics. It provides concise, real-world agricultural advice in English. This dataset is ideal for training chatbots that deliver clear, actionable farming tips to support farmers with quick, easy-to-understand guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7pcRLitUrf5"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries and load dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-pRutEWD1o8",
    "outputId": "61ce9de7-8cf3-4ad3-a361-783dfb43634f"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries and load dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "dataset = load_dataset(\"KisanVaani/agriculture-qa-english-only\", split='train')\n",
    "\n",
    "print(f\"Total samples in dataset: {len(dataset)}\")\n",
    "print(dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WR36E_gT6Rb",
    "outputId": "f747a461-34ed-43ba-8211-bc36487e644f"
   },
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for cleaning\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Dataset Cleaning:\n",
    "# 1. Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# 2. Handle missing values - drop rows with missing values as example\n",
    "df = df.dropna()\n",
    "\n",
    "# 3. Standardize string columns to lowercase if applicable\n",
    "# Example: If you have a 'question' column\n",
    "df['question'] = df['question'].str.lower()\n",
    "\n",
    "# 4. Remove outliers (if numerical columns exist, adjust names)\n",
    "# Example below assumes numeric columns are present\n",
    "if len(df.select_dtypes(include=[np.number]).columns) > 0:\n",
    "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "    df = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "print(f\"Samples after cleaning: {len(df)}\")\n",
    "\n",
    "# Convert cleaned DataFrame back to Hugging Face dataset if needed\n",
    "from datasets import Dataset\n",
    "cleaned_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Save cleaned dataset as CSV in 'data' folder\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/cleaned_agri_dataset.csv', index=False)\n",
    "\n",
    "print(\"Cleaned dataset saved to data/cleaned_agri_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M6tk648aaAq"
   },
   "source": [
    "This code splits the dataset into training and validation sets using an 80-20 ratio with a fixed random seed for reproducibility.and cleans the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLQ0aMZBD-Re",
    "outputId": "6932db75-8cc9-49fe-8965-d9dfc9454ede"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Prepare train and validation splits using datasets library\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_data = split_dataset['train']\n",
    "val_data = split_dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "\n",
    "# Convert to pandas DataFrames for saving\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "# Save to data folder\n",
    "os.makedirs('data', exist_ok=True)\n",
    "train_df.to_csv('data/train_split.csv', index=False)\n",
    "val_df.to_csv('data/val_split.csv', index=False)\n",
    "\n",
    "print(\"Train and validation splits saved to data/train_split.csv and data/val_split.csv respectively.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsdPhxgsafrh"
   },
   "source": [
    "# Cell 4: Load tokenizer and define preprocessing\n",
    "The code defines how to load a T5 tokenizer, preprocess data by tokenizing questions and answers, shift decoder tokens, and prepare datasets for training with the T5 model, which is an encoder-decoder transformer for NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "ece8d5c5a21941b585a26b2b3ce1a945",
      "e07107ffc0b242a5b7d34afce7091d14",
      "8068c26c0f924bb38a17e82e25decd85",
      "f402dedcee5649f58f7a7f6bdcd42436",
      "80473fbc4bbb4c27a0565d43e32cf444",
      "b12166e8c85b4c8fae4dead24483d0c8",
      "3d3509b2c9c744de82d4d147a01c189f",
      "1b27724dd28041ddb3d2cc0fc827fb9e",
      "d5b60c2451f449c8a8bf702ae6ee0c89",
      "abdd74fa9e6d46c5b324d8f31badf386",
      "d5e20115d534460181fd83d376e7c4e8",
      "b2b03f9060d14213920d0f25fb598a92",
      "ebc012ab2e4a4f148cdb48db19e3b92c",
      "f8d2ccb85ac945e0a570c53da69f8253",
      "8a47a6281b2f40eaa73f2a39abb6b4d2",
      "c6a8e59b45bc477290997d45fdd9ff62",
      "a535b977be0e48fb874717285f96a4a0",
      "cc47e10a688547a0bbf02f2a1ce2c018",
      "64c6e4d1b2b94ea78a82bda5ecfcae3e",
      "f85491edbff0423998e5d5f7ee567068",
      "be780024732d46a29939e2dabec66b1e",
      "893574d6b39a4eb5ac3f42d48caf1023",
      "b2faa946759f4a7b87fa7256b8451e0b",
      "ec76ace873b944959390de8dbcf68c9f",
      "3a87eb4b969043e78292f9ff5c3293fa",
      "d55ebe53a57e49ee915bf5b42c90f448",
      "c2ed6fabc6f9459ebda0b2a0ef50aeb4",
      "f5289d74f63e4ccca016ea79a1e1cdda",
      "3580cc71907d4cf19d5cd34963ab701e",
      "71e65fda4f164523bc60fcc1cea664d6",
      "14e8f30436a74c57b430c61b8699241e",
      "edd3547df4b54e1bb1f8ff309be24871",
      "0638ed6211894d61ab94179ac3e2423f",
      "c8acf1d61f07444988c39b376f2572d4",
      "b357126116e340d4beb368847412a6db",
      "695750b2769944ac9617b52b405df3bd",
      "f11678ecd81149c89d474f7868c10215",
      "6c39e2db3b474e199a24352eb78b27c9",
      "b187dfe565fe4a14a4c1fc6d3638bb0d",
      "0bd8cab0c49d436199088b8f08ce6036",
      "a8ea692dc73e4bb0a01b71675719b7d5",
      "cb6fb584e5c24ecdb5f27797f25b5c58",
      "f9e880a12aea40a6bfcc960dd105fc90",
      "37aaa4415706412b85d548564e2bc6a2",
      "f30d18c25e8e445380fb6b23d6e913c4",
      "195184fe0590462d92e9fb2153293559",
      "d711703f546143d585613d28391cdbd9",
      "06fe425e22f24fa09cf1f5755139e4cc",
      "65f54e1ee30844b0b5d7afd812ca84a7",
      "e3956f76a320466f9c162f69ad4d8c5b",
      "09eda2b107a14b5c8355c19ec774cb44",
      "156ba90955d54dfda8ebe0c81ae720f8",
      "b5860de55f874bbea70ce65080456724",
      "4595084a27064698a15d3471e9adb4f5",
      "8fdfbd7242dc492aa727044b5fc12c11"
     ]
    },
    "id": "i48Y1ANZECPr",
    "outputId": "b81a0a28-9427-4b06-d573-cf4defae2355"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Load tokenizer and define preprocessing\n",
    "\n",
    "MODEL_NAME = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_LEN_INPUT = 64\n",
    "MAX_LEN_OUTPUT = 64\n",
    "\n",
    "def shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id):\n",
    "    shifted_input_ids = np.zeros_like(input_ids)\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "    shifted_input_ids[input_ids == pad_token_id] = pad_token_id\n",
    "    return shifted_input_ids.tolist()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"question: \" + q for q in examples[\"question\"]]\n",
    "    targets = examples[\"answers\"]  # or 'answer' based on your dataset\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_LEN_INPUT, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(targets, max_length=MAX_LEN_OUTPUT, truncation=True, padding=\"max_length\")\n",
    "    labels_ids = labels[\"input_ids\"]\n",
    "\n",
    "    labels_ids = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels_ids]\n",
    "    model_inputs[\"labels\"] = labels_ids\n",
    "\n",
    "    decoder_input_ids = shift_tokens_right(\n",
    "        np.array(labels[\"input_ids\"]),\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        decoder_start_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_data.map(preprocess_function, batched=True)\n",
    "val_dataset = val_data.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"tensorflow\", columns=['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n",
    "val_dataset.set_format(type=\"tensorflow\", columns=['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ5H8k1sanYA"
   },
   "source": [
    "I  used Hugging Face’s datasets and tokenizer to prepare my data, then converting it into TensorFlow datasets for smooth integration with TensorFlow training. This way, I can efficiently feed batches of tokenized text and labels into my model during training. It helps me combine Hugging Face’s flexible preprocessing with TensorFlow’s powerful training pipeline in just a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUdv0TvQEFnT",
    "outputId": "bb3e0779-a091-4f82-a22b-57230e75dc01"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Convert Hugging Face datasets to tf.data.Dataset, including decoder_input_ids\n",
    "\n",
    "def to_tf_dataset(hf_dataset):\n",
    "    features = {\n",
    "        \"input_ids\": np.array(hf_dataset[\"input_ids\"]),\n",
    "        \"attention_mask\": np.array(hf_dataset[\"attention_mask\"]),\n",
    "        \"decoder_input_ids\": np.array(hf_dataset[\"decoder_input_ids\"]),\n",
    "    }\n",
    "    labels = np.array(hf_dataset[\"labels\"])\n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "tf_train_dataset = to_tf_dataset(train_dataset).shuffle(100).batch(batch_size)\n",
    "tf_val_dataset = to_tf_dataset(val_dataset).batch(batch_size)\n",
    "\n",
    "print(\"Training dataset batches:\", tf_train_dataset)\n",
    "print(\"Validation dataset batches:\", tf_val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kozv1jlPassE"
   },
   "source": [
    "I loaded the pretrained T5 model using TensorFlow and initialize the Adam optimizer. Then, I compile the model with a custom loss that ignores padding tokens, preparing it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "8944f7d7bebc4671be3ea3908b896e0c",
      "823e08e98ef04a40b8e2b3f05eccd286",
      "1136746c1afe437f817cf0d0c235f990",
      "7e1fee6e13d14d169c5a2729540a08a3",
      "6167c754892349b197aa97a1b8435a1e",
      "59fdee4894004b178416c00edfa811e6",
      "6d9a4540705d42dabcd0b6e1db5abd5f",
      "51fffdc739444de6bc455c5bd289bc6d",
      "e3adc898d75145f7a058e67e058f2c4a",
      "4700d55be41f4498b976404461605c79",
      "bfdbbed5aea843579d49257221fac6a1",
      "a5bd127a0cb243da912b6eed93e7c5b0",
      "cb76744306c440b78962afb77bd89cd4",
      "99e2bca151674fb380bc55b7ec9d3902",
      "49d406d31a734bbaa28f4d040153196d",
      "a41db8e9c300420abf862eb43eb5ae4e",
      "4fde18f88b7c4185b0b03df3643996b1",
      "420196c44ba54d3992d16f9d02c13c9d",
      "dc0ede4c0a2143b59919df49032c4cb7",
      "5dc26026f6b34d30a22ed7d813051e73",
      "6e6294128fa04d73a29be77cd13903d0",
      "7fa05dc3d2f94e02bd7504afd9a2b006"
     ]
    },
    "id": "NMAVudAcEIng",
    "outputId": "6f58d994-d8f2-47fc-f0f2-49e8c6321fd3"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Load model and compile\n",
    "\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(MODEL_NAME, from_pt=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    pad_token_id = tf.constant(tokenizer.pad_token_id, dtype=y_true.dtype)\n",
    "    y_true_safe = tf.where(y_true == -100, pad_token_id, y_true)\n",
    "    loss_ = tf.keras.losses.sparse_categorical_crossentropy(y_true_safe, y_pred, from_logits=True)\n",
    "    mask = tf.cast(tf.not_equal(y_true, -100), dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=masked_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx-A6hn9a0wu"
   },
   "source": [
    "I train the model for 3 epochs using the TensorFlow training and validation datasets. Then, I plot the training and validation loss curves to monitor the model’s performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "I7C9eAvxEL0v",
    "outputId": "f657decf-e24f-4cbc-daab-c9889d93fc4d"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Train model\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = model.fit(tf_train_dataset, validation_data=tf_val_dataset, epochs=epochs)\n",
    "\n",
    "# Plot training/validation loss\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6BqVuS6a7Yg"
   },
   "source": [
    "saving model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1Bjz9utEPjY",
    "outputId": "df479694-9ec1-4b75-b7b4-860220adf62b"
   },
   "outputs": [],
   "source": [
    "# Cell 8: Save model and tokenizer\n",
    "\n",
    "model.save_pretrained(\"./agri_chatbot_t5\")\n",
    "tokenizer.save_pretrained(\"./agri_chatbot_t5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgh-VQ_SES-t",
    "outputId": "73cde85a-4d71-4fbc-a6d5-6d2fbd9b2ea3"
   },
   "outputs": [],
   "source": [
    "# Cell 9: Define inference function and test\n",
    "\n",
    "def generate_answer(question, max_length=64):\n",
    "    input_text = \"question: \" + question\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"tf\", max_length=MAX_LEN_INPUT, truncation=True, padding=\"max_length\")\n",
    "    outputs = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "test_question = \"what is organic farming?\"\n",
    "print(\"Q:\", test_question)\n",
    "print(\"A:\", generate_answer(test_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Th1hsYhK0UA2",
    "outputId": "b1a03157-f8d6-48b4-b43c-b0ab8602652f"
   },
   "outputs": [],
   "source": [
    "question = \"what is fertilizers?\"\n",
    "answer = generate_answer(question)\n",
    "print(\"Q:\", question)\n",
    "print(\"A:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8op7yNBxdEQP",
    "outputId": "2471f94a-3d52-48ff-ff82-9aedfc8e5385"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout\n",
    "!nbstripout Notebook.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jW9V1scHe18c",
    "outputId": "d15ebc63-3935-4732-d04c-94f997f79503"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # select Notebook.ipynb from your computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWDxR-pgetd9"
   },
   "outputs": [],
   "source": [
    "!git add my_notebook.ipynb\n",
    "!git commit -m \"Clean notebook metadata for GitHub\"\n",
    "!git push https://<YOUR_TOKEN>@github.com/nellyiya/CHATBOT.git\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
